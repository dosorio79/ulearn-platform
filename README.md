# uLearn — Agentic Micro-Learning Platform

[![CI](https://img.shields.io/badge/CI-pending-lightgrey)](#)
[![License](https://img.shields.io/badge/License-MIT-green)](#)
[![Docker](https://img.shields.io/badge/Docker-ready-blue)](#)

> Focused 15‑minute lessons on demand, generated by a multi‑agent backend and rendered for a calm, fast learning flow.

![Product screenshot](docs/assets/screenshot.png)

> Screenshot placeholder: add `docs/assets/screenshot.png` when ready.

## Problem statement

Data practitioners frequently need to learn or refresh a specific concept quickly, but most educational resources are long, unfocused, or poorly adapted to short learning sessions.

This project addresses that problem by generating **focused 15-minute lessons on demand**, tailored to a given topic and difficulty level.

---

## What the system does

The system:

* Accepts a topic and difficulty level from the user
* Uses multiple AI agents to:

  * Plan a pedagogically scoped lesson
  * Generate structured content blocks
  * Validate lesson structure and time constraints
* Renders structured blocks into Markdown for the frontend
* Logs each lesson generation run for telemetry purposes

The system is stateless from a user perspective, but includes a persistence layer for session-level logging and analysis.

---

## High-level architecture

```
Frontend (React)
        |
        v
     POST /lesson
        |
        v
FastAPI Backend
 ├── Agent orchestration
 ├── OpenAPI contract
 └── MongoDB telemetry
        |
        v
     MongoDB
```

---

## API contract

The backend exposes a single public endpoint:

* `POST /lesson`

The API contract is defined using **OpenAPI** (`openapi.yaml`) and serves as the single source of truth for frontend and backend development.

The frontend follows this contract for real HTTP calls to the backend.

---

## Frontend code execution

The frontend can execute Python snippets in lessons using **Pyodide** (running in the browser).

* Python code blocks include a *Run* button and display stdout output
* If a snippet does not produce output, the UI prompts the learner to add a `print(...)` statement

You can override the Pyodide base URL with `VITE_PYODIDE_BASE` (defaults to the jsDelivr CDN).

---

## Agent-based workflow

Lesson generation is implemented using multiple cooperating agents:

* **PlannerAgent** – defines lesson structure and time budget
* **ContentAgent** – generates structured content blocks (stub implementation)
* **ContentAgentLLM** – optional LLM-backed content generator
* **ValidatorAgent** – enforces structure (required sections, block formatting) and normalizes section minutes to a 15-minute total

Only the content generation step uses an LLM; planning and validation are deterministic and fully testable.

Prompt sources for the LLM content agent:

* System prompt: `app/agents/prompts/content_llm_system.txt`
* User prompt template: `app/agents/prompts/content_llm_user.txt`

See `docs/prompts.md` for editing guidance.

### Validation philosophy

Validation is intentionally strict to guarantee:

* Predictable lesson structure
* Executable Python code blocks
* Stable frontend rendering

Frontend UX includes:

* Per-section copy buttons
* Full-lesson Markdown export
* Feedback prompt and improved loading/error states

A detailed description of agent responsibilities and orchestration is available in `docs/agent-architecture.md`.
The lesson format proof-of-concept and validator rule candidates are summarized in `docs/lesson-generation-poc.md`.

Rules and constraints for AI-assisted development are defined in `AGENTS.md`.

---

## Telemetry and persistence

Each lesson generation run is logged to MongoDB, including:

* Session identifier
* Request parameters
* Output summary (objective, total minutes, section ids)
* Timestamp

Failure cases (schema validation, content validation, unexpected exceptions) are recorded as separate failure documents for post-mortem analysis.
Failure records capture the error type, summary message, and optional schema error details.

This telemetry-first design mirrors authenticated usage patterns and allows a future transition to authenticated user sessions without schema changes.

---

## Technologies used

* Frontend: React
* Backend: FastAPI (Python 3.12)
* AI: Large Language Model (LLM) orchestrated via `pydantic-ai`
* Database: MongoDB
* API specification: OpenAPI 3.0
* Containerization: Docker, docker-compose
* Dependency management: uv

---

## Running the project locally

### Requirements

* Docker and docker-compose
* Python 3.12+
* uv
* Node.js 18+

### Quickstart

```bash
make build
make start
```

Then open:

* http://localhost:8080 (UI)
* http://localhost:8000/docs (API)

### Start the system

```bash
docker compose up --build
```

This starts the backend, MongoDB, and the frontend container.

> Note: the frontend image expects a prebuilt `frontend/dist`.
> Run `npm run build` in `frontend/` before building the containers.

Quick start (recommended):

```bash
make build
```

If you've already built the images:

```bash
make start
```

For Docker, the frontend container proxies `/lesson` and `/health` to the backend.
You can leave `API_BASE` empty in `frontend/public/runtime-config.js` to use same-origin requests.

To call the backend directly, set:

```
API_BASE=http://localhost:8000
```

### Available services

* Backend API: [http://localhost:8000](http://localhost:8000)
* OpenAPI docs: [http://localhost:8000/docs](http://localhost:8000/docs)
* Frontend UI: [http://localhost:8080](http://localhost:8080)

---

## Configuration

Create your environment file from `.env-example` and update values as needed:

* `OPENAI_API_KEY`: required when `USE_LLM_CONTENT=true`
* `MODEL`: LLM model name (defaults to `gpt-4.1-mini`)
* `USE_LLM_CONTENT`: toggle LLM-backed content generation (`true` / `false`)
* `CORS_ORIGINS`: allowed origins (default: `http://localhost:8080`)
* `MONGO_FAILURE_COLLECTION`: MongoDB collection for failure telemetry (default: `lesson_failures`)

---

## Testing

Tests cover:

* Agent planning and validation logic
* Lesson orchestration
* API contract and integration workflow

Run all tests:

```bash
uv run pytest
```

Run specific categories with markers:

```bash
pytest -m unit
pytest -m llm
pytest -m api
pytest -m integration
```

Frontend tests:

```bash
cd frontend
npm test
```

Makefile shortcuts:

```bash
make test
make test-unit
make test-api
make test-llm
make test-integration
```

---

## Non-goals (by design)

The following are intentionally excluded to keep the system focused:

* User authentication
* Long-term user profiles
* Adaptive curricula
* RAG / document ingestion
* Production-grade content evaluation

These are natural extensions but outside the scope of this project.

---

## Future work

* Authentication and user accounts
* Lesson personalization
* Automated exercise evaluation
* Learning analytics and feedback loops

---

## Changelog

See `CHANGELOG.md` for release notes.
