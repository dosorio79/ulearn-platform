# uLearn — Agentic Micro-Learning Platform

[![CI](https://github.com/dosorio79/ulearn-platform/actions/workflows/ci.yml/badge.svg)](https://github.com/dosorio79/ulearn-platform/actions/workflows/ci.yml)
[![License](https://img.shields.io/badge/License-MIT-green)](#)
[![Docker](https://img.shields.io/badge/Docker-ready-blue)](#)

> Focused 15-minute lessons on demand, generated by a multi-agent backend and rendered for a calm, fast learning experience.

![uLearn – lesson generation UI](docs/assets/ulearn.png)

---

## Problem statement

Data practitioners often need to learn or refresh a specific concept quickly, but most educational resources are long, unfocused, or poorly suited to short learning sessions.

This project addresses that problem by generating **focused 15-minute lessons on demand**, tailored to a given topic and difficulty level.

---

## What the system does

The system:

- Accepts a topic and difficulty level from the user
- Uses multiple AI agents to:
  - Plan a pedagogically scoped lesson
  - Generate structured content blocks
  - Validate lesson structure and enforce time constraints
- Renders structured blocks into Markdown for the frontend
- Logs each lesson generation run for telemetry purposes

The system is stateless from a user perspective; persistence is used exclusively for session-level logging and analysis.

---

## High-level architecture

Frontend (React)  
↓  
POST /lesson  
↓  
FastAPI Backend  
- Agent orchestration  
- OpenAPI contract  
- Telemetry backend  
↓  
MongoDB / In-memory

### Architecture diagram

![uLearn architecture](docs/assets/architecture.png)

---

## Agent-based workflow

Lesson generation is implemented using multiple cooperating agents:

- **PlannerAgent** – defines lesson structure and allocates a 15-minute time budget based on difficulty
- **ContentAgent** – generates structured content blocks (stub implementation)
- **ContentAgentLLM** – optional LLM-backed content generator
- **ValidatorAgent** – enforces structure (required sections, block formatting) and normalizes section durations

Only the content generation step may use an LLM; planning and validation are deterministic, fully testable, and independent of model behavior.

Prompt sources for the LLM-backed content agent:

- System prompt: `app/agents/prompts/content_llm_system.txt`
- User prompt template: `app/agents/prompts/content_llm_user.txt`

See `docs/prompts.md` for editing guidance.

### Validation philosophy

Validation is intentionally strict to guarantee the following properties:

- Predictable lesson structure
- Executable Python code blocks
- Stable frontend rendering

Frontend UX includes:

- Per-section copy buttons
- Full-lesson Markdown and Jupyter notebook export
- Feedback prompts and improved loading/error states

Detailed agent responsibilities are documented in `docs/agent-architecture.md`.  
Lesson format proof-of-concept and validator rules are summarized in `docs/lesson-generation-poc.md`.  
AI-assisted development constraints are defined in `AGENTS.md`.

---

## V1 goals (planned)

The V1 focus is on tool-augmented validation and learning-quality telemetry:

- MCP-backed content validation (tools arbitrate correctness)
- Explicit content contracts (schema + semantic constraints)
- First-class telemetry for learning quality, not just errors
- Deterministic lesson rebuilds for testing and review
- Clear agent boundaries (planner, content, validator, MCP tools)

See `docs/v1-goals.md` for full details and near-term steps.

---

## Frontend code execution

The frontend can execute Python snippets in lessons using **Pyodide** (running in the browser).

- Python code blocks include a *Run* button and display stdout output
- If a snippet does not produce output, the UI prompts the learner to add `print(...)`
- Python blocks are read-only by default; use *Edit* for quick fixes and *Reset* to restore the original snippet
- Exports always use the original generated lesson content

You can override the Pyodide base URL with `VITE_PYODIDE_BASE` (defaults to the jsDelivr CDN).

---

## Telemetry and persistence

Each lesson generation run is logged, including:

- Session identifier
- Request parameters
- Output summary (objective, total minutes, section IDs)
- Attempt count (number of generation attempts needed)
- Timestamp

Failure cases (schema validation, content validation, unexpected exceptions) are recorded separately for post-mortem analysis.

Telemetry is designed for system evaluation and iteration, not for user tracking or personalization.

---

## Technologies used

- Backend: FastAPI (Python 3.12)
- Frontend: React
- AI orchestration: `pydantic-ai`
- API specification: OpenAPI 3.0
- Database: MongoDB (optional)
- Containerization: Docker, docker-compose
- Dependency management: uv

---

## Running the project locally

### Requirements

- Docker and docker-compose
- Python 3.12+
- uv
- Node.js 18+

### Quickstart

```bash
make build
make start
```

Then open:

- http://localhost:8080 — Frontend UI  
- http://localhost:8000/docs — API documentation

### Start manually

```bash
docker compose up --build
```

> Note: the frontend image expects a prebuilt `frontend/dist`.  
> Run `npm run build` in `frontend/` before building the containers.

To call the backend directly, set:

```bash
API_BASE=http://localhost:8000
```

---

## Configuration

Create your environment file from `.env-example` and update values as needed:

- `OPENAI_API_KEY` – required when `USE_LLM_CONTENT=true`
- `MODEL` – LLM model name (default: `gpt-4.1-mini`)
- `USE_LLM_CONTENT` – toggle LLM-backed content generation
- `CORS_ORIGINS` – allowed origins (default: `http://localhost:8080`)
- `MONGO_FAILURE_COLLECTION` – MongoDB collection for failure telemetry
- `STATIC_LESSON_MODE` – serve static lesson templates
- `TELEMETRY_BACKEND` – `mongo` or `memory`
- `DEMO_MODE` – shorthand for static lessons + memory telemetry
- `TELEMETRY_MEMORY_CAP` – max in-memory telemetry entries

---

## Deployment (Render demo)

This repository includes a **Render Blueprint–based deployment** designed for **continuous deployment of a self-contained demo**, without external dependencies (LLM APIs or MongoDB).

**Live demo (auto-deploys from `main`)**  
https://ulearn-frontend.onrender.com/

### What is deployed

- **Backend (FastAPI)**  
  Runs in demo mode:
  - `STATIC_LESSON_MODE=true`
  - `TELEMETRY_BACKEND=memory`
  - No LLM calls
  - No MongoDB dependency

- **Frontend (static React build)**  
  Served as a static site, calling the backend API via `API_BASE`.

### What is NOT deployed (by design)

- LLM-backed content generation
- MongoDB persistence
- Authenticated user flows

These features are available locally and documented above, but excluded from the Render deployment to keep the demo reliable and cost-free:
- The full LLM flow requires a valid `OPENAI_API_KEY`.
- The production dataset/content setup is non-trivial to provision in a public demo environment.

This setup still demonstrates **continuous deployment**: Render auto-deploys the demo on every push to `main`, while CI runs tests on push/PR.

### Deploying on Render

1. Create a new **Render Blueprint** from `render.yaml`
2. Deploy the backend service first and copy its public URL
3. Set the following value in `render.yaml`:

   ```bash
   API_BASE=<backend-url>
   ```

4. Deploy the frontend service

Once deployed:

- Frontend URL → demo UI  
- Backend `/docs` → live OpenAPI specification

### Local demo parity

```bash
cp .env.render .env
docker compose up --build
```

---

## Testing

Tests cover core system behavior, including:

- Agent planning and validation logic
- Lesson orchestration
- API contract and integration workflow

Run all tests:

```bash
uv run pytest
```

Frontend tests:

```bash
cd frontend
npm test
```

---

## CI

GitHub Actions runs backend tests (with a MongoDB service) and frontend lint/test/build on every push and pull request.  
CI validates correctness; CD is handled via the Render Blueprint demo deployment, which auto-deploys from `main`.

---

## Non-goals (by design)

- User authentication
- Long-term user profiles
- Adaptive curricula
- RAG / document ingestion
- Production-grade content evaluation

---

## Future work

- Authentication and user accounts
- Lesson personalization
- Automated exercise evaluation
- Learning analytics and feedback loops

---

## Changelog

See `CHANGELOG.md` for release notes.

---

## Repository history note

This repository was extracted from a larger monorepo used during the AI Dev Tools Zoomcamp course.

- Git history prior to version `v0.3.0` does not reflect the original course repository
- Commits from `v0.3.0` onward represent the standalone evolution of this project

This restructuring enables:

- Clear ownership and scope
- Independent versioning and releases
- CI/CD and deployment workflows specific to this project
